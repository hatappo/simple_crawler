
#
# クローラのhttpヘッダに指定する内容です。
#
headers = {
  "accept-language": "ja,en-US;q=0.8,en;q=0.6",
  "cache-control":   "max-age=0",
}


#
# クローラのhttpヘッダに指定するユーザーエージェントです。
# ここでユーザーエージェントを指定した場合 headers でユーザーエージェントを指定していてもその設定を上書きします。
#
user_agent = "Sample bot"


#
# クローリングの1ページ毎の処理の待ち（Kernel#sleep）時間（秒）です。
#
wait_sec_by_each_page = 0.05


#
# メインのクローリングの失敗を許容する回数です。
# クローリングの失敗（URLへのアクセスの失敗）がこの数に達すると処理を中断して終了します。0 の場合は無限に許容します。
#
invalid_crawling_url_cnt_threashold = 1

#
# スクレイピングした結果を出力するファイルです。
#
#scraping_out = "./sample.conf.out"


#
# クローリングするURL一覧。以下２つの指定方法が可能です。
#   String : URL一覧を収めたファイルをファイルパスで指定します。
#    Array : URL一覧を直接列挙して指定します。
#
crawling_list = ["it", "economics", "knowledge"].map {|product| "http://b.hatena.ne.jp/hotentry/#{product}"}


#
# スクレイピング処理。
# Capybara::Session を使ってページ上から適宜情報を抜き出し out に出力します。
# ライターの出力先は scraping_out で指定します。
#
scraping {|session, logger, out|
  session.all("h3.hb-entry-link-container > a").each.with_index(1) do |elem, idx|
    logger.info "---- #{idx}こ目\t#{elem[:title]} " 
  end
}


#
# クローリング処理。
# Capybara::Session を使ってページ上から適宜情報を抜き出し、次の遷移先 status[:next_url] をセットします。
# 現在のページからたどる遷移先がない場合は status[:next_url] はセットしません。
#
crawling {|session, logger, status|
  logger.debug "レスポンスヘッダ = #{session.response_headers}"
}


# クローリング処理とスクレイピング処理で使えるのAPIについては、以下が参考になります。
# @see http://www.rubydoc.info/github/jnicklas/capybara/Capybara/Session
# @see https://github.com/jnicklas/capybara#using-the-dsl-elsewhere
